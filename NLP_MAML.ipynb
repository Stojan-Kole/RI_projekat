{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:14.812160Z",
     "start_time": "2025-02-04T08:34:14.804203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb"
   ],
   "id": "1d223714e7b9977c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:15.003230Z",
     "start_time": "2025-02-04T08:34:14.981151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"maml-fewshot-multiset\",\n",
    "    name=\"maml-multiset-manual\",\n",
    ")"
   ],
   "id": "da5df5b9973dfc16",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kostic-stojan23-university-of-belgrade/maml-fewshot-multiset/runs/s50nr2sa?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1576a418e60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:15.185319Z",
     "start_time": "2025-02-04T08:34:15.175057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_normalize(dataset_name):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    if dataset_name == \"amazon_polarity\":\n",
    "        def process(example):\n",
    "            return {\"text\": example[\"content\"], \"label\": example[\"label\"]}\n",
    "    elif dataset_name == \"yelp_polarity\":\n",
    "        def process(example):\n",
    "            return {\"text\": example[\"text\"], \"label\": example[\"label\"]}\n",
    "    elif dataset_name == \"sentiment140\":\n",
    "        def process(example):\n",
    "            label = 0 if example[\"sentiment\"] == 0 else 1  # Convert (0,4) to (0,1)\n",
    "            return {\"text\": example[\"text\"], \"label\": label}\n",
    "\n",
    "    dataset[\"train\"] = dataset[\"train\"].map(process)\n",
    "    dataset[\"test\"] = dataset[\"test\"].map(process)\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"test\"]"
   ],
   "id": "7ef60eb3bcbaf04e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:37.574328Z",
     "start_time": "2025-02-04T08:34:15.348083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_names = [\"amazon_polarity\", \"yelp_polarity\", \"sentiment140\"]\n",
    "train_data, test_data = {}, {}\n",
    "\n",
    "for name in dataset_names:\n",
    "    train_data[name], test_data[name] = load_and_normalize(name)"
   ],
   "id": "252b4ccaf47d5f6f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:37.586769Z",
     "start_time": "2025-02-04T08:34:37.576846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, data, num_support, num_query):\n",
    "        self.data = data\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "\n",
    "    def get_task(self):\n",
    "        indices = list(range(len(self.data)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        support_indices = indices[:self.num_support]\n",
    "        query_indices = indices[self.num_support:self.num_support + self.num_query]\n",
    "\n",
    "        support_set = [(self.data[i]['text'], self.data[i]['label']) for i in support_indices]\n",
    "        query_set = [(self.data[i]['text'], self.data[i]['label']) for i in query_indices]\n",
    "\n",
    "        return support_set, query_set"
   ],
   "id": "859ce03cf1b6b634",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:37.911795Z",
     "start_time": "2025-02-04T08:34:37.635783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"NLP_VER1\"\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "id": "7e5b949d8dd1d9b9",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:38.663279Z",
     "start_time": "2025-02-04T08:34:37.962445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model.to(device)\n",
    "\n",
    "def inner_loop(model, support_set, num_steps=1, lr=1e-5):\n",
    "    task_model = deepcopy(model).to(device)\n",
    "    optimizer = torch.optim.Adam(task_model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        for text, label in support_set:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            labels = torch.tensor([label]).unsqueeze(0).to(device)  # Move labels to GPU\n",
    "\n",
    "            outputs = task_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return task_model"
   ],
   "id": "4d465330344cc7e5",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:38.732675Z",
     "start_time": "2025-02-04T08:34:38.724225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def outer_loop(meta_model, tasks, meta_optimizer, num_inner_steps=1, lr = 1e-5):\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "\n",
    "    for support_set, query_set in tasks:\n",
    "        task_model = inner_loop(meta_model, support_set, num_steps=num_inner_steps, lr=lr)\n",
    "\n",
    "        for text, label in query_set:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            labels = torch.tensor([label]).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = task_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(meta_model.parameters(), max_norm=0.5)\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    return total_loss.item()"
   ],
   "id": "e14ed62ddf11eb0d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:34:38.815605Z",
     "start_time": "2025-02-04T08:34:38.807972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meta_lr = 1e-5\n",
    "inner_lr = 1e-6\n",
    "num_support = 5\n",
    "num_query = 5\n",
    "inner_steps = 5\n",
    "batch_size = 4\n",
    "meta_epochs = 20"
   ],
   "id": "4ddfd6735ed70a86",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:27:55.442154Z",
     "start_time": "2025-02-04T08:34:38.865340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_datasets = {\n",
    "    name: FewShotDataset(train_data[name], num_support, num_query)\n",
    "    for name in dataset_names\n",
    "}\n",
    "\n",
    "meta_optimizer = Adam(base_model.parameters(), lr=meta_lr)\n",
    "\n",
    "for epoch in range(meta_epochs):\n",
    "    tasks = []\n",
    "    for name in dataset_names:\n",
    "        tasks.extend([few_shot_datasets[name].get_task() for _ in range(batch_size)])\n",
    "\n",
    "    loss = outer_loop(base_model, tasks, meta_optimizer, num_inner_steps=inner_steps, lr=inner_lr)\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"meta_loss\": loss,\n",
    "        \"meta_lr\": meta_lr,\n",
    "        \"inner_lr\": inner_lr,\n",
    "        \"num_support\": num_support,\n",
    "        \"num_query\": num_query,\n",
    "        \"inner_steps\": inner_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Meta Loss: {loss:.4f}\")"
   ],
   "id": "c23c3f9f27075c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Meta Loss: 39.6423\n",
      "Epoch 2, Meta Loss: 37.4839\n",
      "Epoch 3, Meta Loss: 42.3168\n",
      "Epoch 4, Meta Loss: 21.7572\n",
      "Epoch 5, Meta Loss: 30.6382\n",
      "Epoch 6, Meta Loss: 32.8623\n",
      "Epoch 7, Meta Loss: 31.2697\n",
      "Epoch 8, Meta Loss: 30.9950\n",
      "Epoch 9, Meta Loss: 17.8127\n",
      "Epoch 10, Meta Loss: 23.9224\n",
      "Epoch 11, Meta Loss: 21.5387\n",
      "Epoch 12, Meta Loss: 23.4956\n",
      "Epoch 13, Meta Loss: 19.8614\n",
      "Epoch 14, Meta Loss: 32.4785\n",
      "Epoch 15, Meta Loss: 22.6566\n",
      "Epoch 16, Meta Loss: 28.3319\n",
      "Epoch 17, Meta Loss: 34.7042\n",
      "Epoch 18, Meta Loss: 19.4609\n",
      "Epoch 19, Meta Loss: 27.8355\n",
      "Epoch 20, Meta Loss: 20.2789\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:28:28.905004Z",
     "start_time": "2025-02-04T09:27:55.593226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset_name in dataset_names:\n",
    "    new_task_data = FewShotDataset(test_data[dataset_name], num_support, num_query)\n",
    "    new_support_set, new_query_set = new_task_data.get_task()\n",
    "\n",
    "    adapted_model = inner_loop(base_model, new_support_set, num_steps=inner_steps)\n",
    "    adapted_model = adapted_model.to(device)\n",
    "    total_loss = 0\n",
    "\n",
    "    for text, label in new_query_set:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        labels = torch.tensor([label]).unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = adapted_model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss\n",
    "\n",
    "    wandb.log({f\"{dataset_name}_task_loss\": total_loss.item()})\n",
    "    print(f\"Final Evaluation Loss on {dataset_name}: {total_loss.item():.4f}\")"
   ],
   "id": "e277fa3ae2b32175",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Loss on amazon_polarity: 0.1861\n",
      "Final Evaluation Loss on yelp_polarity: 0.2266\n",
      "Final Evaluation Loss on sentiment140: 4.6426\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T09:28:29.646256Z",
     "start_time": "2025-02-04T09:28:28.931879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 20 unseen sentences for testing\n",
    "test_sentences = [\n",
    "    \"The customer service was beyond my expectations!\",\n",
    "    \"I waited an hour for my order, and it was still wrong.\",\n",
    "    \"This new phone update is a complete disaster.\",\n",
    "    \"I'm absolutely thrilled with my new laptop!\",\n",
    "    \"The food was bland and overpriced, not coming back.\",\n",
    "    \"Best vacation spot ever, can't wait to return!\",\n",
    "    \"The product broke after just two uses, very disappointed.\",\n",
    "    \"Excellent book, well-written and engaging.\",\n",
    "    \"Movie was predictable and boring, nothing special.\",\n",
    "    \"Customer support was extremely helpful and quick to respond.\",\n",
    "    \"I regret purchasing this item, waste of money.\",\n",
    "    \"One of the best restaurants in town, highly recommended!\",\n",
    "    \"The new policy changes are frustrating and unnecessary.\",\n",
    "    \"I love how comfortable and stylish these shoes are!\",\n",
    "    \"The concert was an unforgettable experience.\",\n",
    "    \"The software crashes frequently, making it unusable.\",\n",
    "    \"Brilliant storytelling, kept me hooked from start to finish.\",\n",
    "    \"Shipping took forever, and the package arrived damaged.\",\n",
    "    \"Great workout program, helped me get in shape quickly.\",\n",
    "    \"The app's interface is confusing and hard to navigate.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = adapted_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "if isinstance(predictions, torch.Tensor):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "for i, (sentence, prob) in enumerate(zip(test_sentences, probs)):\n",
    "    print(f\"{i+1}. {sentence}\\n   ➝ Positive: {prob[1]:.2f}, Negative: {prob[0]:.2f}\\n\")\n"
   ],
   "id": "3e13de2e09aab63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The customer service was beyond my expectations!\n",
      "   ➝ Positive: 0.98, Negative: 0.02\n",
      "\n",
      "2. I waited an hour for my order, and it was still wrong.\n",
      "   ➝ Positive: 0.02, Negative: 0.98\n",
      "\n",
      "3. This new phone update is a complete disaster.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "4. I'm absolutely thrilled with my new laptop!\n",
      "   ➝ Positive: 0.99, Negative: 0.01\n",
      "\n",
      "5. The food was bland and overpriced, not coming back.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "6. Best vacation spot ever, can't wait to return!\n",
      "   ➝ Positive: 0.99, Negative: 0.01\n",
      "\n",
      "7. The product broke after just two uses, very disappointed.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "8. Excellent book, well-written and engaging.\n",
      "   ➝ Positive: 1.00, Negative: 0.00\n",
      "\n",
      "9. Movie was predictable and boring, nothing special.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "10. Customer support was extremely helpful and quick to respond.\n",
      "   ➝ Positive: 0.99, Negative: 0.01\n",
      "\n",
      "11. I regret purchasing this item, waste of money.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "12. One of the best restaurants in town, highly recommended!\n",
      "   ➝ Positive: 1.00, Negative: 0.00\n",
      "\n",
      "13. The new policy changes are frustrating and unnecessary.\n",
      "   ➝ Positive: 0.00, Negative: 1.00\n",
      "\n",
      "14. I love how comfortable and stylish these shoes are!\n",
      "   ➝ Positive: 1.00, Negative: 0.00\n",
      "\n",
      "15. The concert was an unforgettable experience.\n",
      "   ➝ Positive: 0.99, Negative: 0.01\n",
      "\n",
      "16. The software crashes frequently, making it unusable.\n",
      "   ➝ Positive: 0.01, Negative: 0.99\n",
      "\n",
      "17. Brilliant storytelling, kept me hooked from start to finish.\n",
      "   ➝ Positive: 1.00, Negative: 0.00\n",
      "\n",
      "18. Shipping took forever, and the package arrived damaged.\n",
      "   ➝ Positive: 0.58, Negative: 0.42\n",
      "\n",
      "19. Great workout program, helped me get in shape quickly.\n",
      "   ➝ Positive: 0.99, Negative: 0.01\n",
      "\n",
      "20. The app's interface is confusing and hard to navigate.\n",
      "   ➝ Positive: 0.48, Negative: 0.52\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:20:36.518469Z",
     "start_time": "2025-02-04T09:28:29.755726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def evaluate_model_on_all_datasets(model, test_data):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for dataset_name in [\"sentiment140\", \"amazon_polarity\", \"yelp_polarity\"]:\n",
    "        encodings = tokenizer(test_data[dataset_name][\"text\"], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "        test_dataset = torch.utils.data.TensorDataset(encodings.input_ids, encodings.attention_mask, torch.tensor(test_data[dataset_name][\"label\"]))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids = input_ids.to(model.device)\n",
    "                attention_mask = attention_mask.to(model.device)\n",
    "                labels = labels.to(model.device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        wandb.log({f\"{dataset_name}_accuracy\": accuracy})\n",
    "\n",
    "        print(f\"Evaluation on {dataset_name}: Accuracy = {accuracy:.4f}\")\n",
    "        print(f\"Classification Report for {dataset_name}:\")\n",
    "        print(classification_report(all_labels, all_preds))\n",
    "\n",
    "evaluate_model_on_all_datasets(adapted_model, test_data)"
   ],
   "id": "450e3d79c9b4c549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on sentiment140: Accuracy = 0.7952\n",
      "Classification Report for sentiment140:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.66       177\n",
      "           1       0.80      0.92      0.85       321\n",
      "\n",
      "    accuracy                           0.80       498\n",
      "   macro avg       0.80      0.74      0.76       498\n",
      "weighted avg       0.80      0.80      0.79       498\n",
      "\n",
      "Evaluation on amazon_polarity: Accuracy = 0.8728\n",
      "Classification Report for amazon_polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86    200177\n",
      "           1       0.82      0.96      0.88    200321\n",
      "\n",
      "    accuracy                           0.87    400498\n",
      "   macro avg       0.88      0.87      0.87    400498\n",
      "weighted avg       0.88      0.87      0.87    400498\n",
      "\n",
      "Evaluation on yelp_polarity: Accuracy = 0.8758\n",
      "Classification Report for yelp_polarity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86    219177\n",
      "           1       0.82      0.96      0.89    219321\n",
      "\n",
      "    accuracy                           0.88    438498\n",
      "   macro avg       0.89      0.88      0.87    438498\n",
      "weighted avg       0.89      0.88      0.87    438498\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T14:20:39.501510Z",
     "start_time": "2025-02-05T14:20:36.652935Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "149ebd6ce2ebbc0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>amazon_polarity_accuracy</td><td>▁</td></tr><tr><td>amazon_polarity_task_loss</td><td>█▁</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▃▃▃▃▃▃▄▄▅▁▁▁▂▂▂▂▃▄▄▄▄▄▄▅▅▆▆▆▇▇██▁▂▂▂▂</td></tr><tr><td>inner_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>inner_steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████</td></tr><tr><td>meta_loss</td><td>█▂▁▂▁▁▁▂▁▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▁▂▁▁▂▂▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>meta_lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_query</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_support</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sentiment140_accuracy</td><td>▁</td></tr><tr><td>sentiment140_task_loss</td><td>▁█</td></tr><tr><td>yelp_polarity_accuracy</td><td>▁</td></tr><tr><td>yelp_polarity_task_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>amazon_polarity_accuracy</td><td>0.87276</td></tr><tr><td>amazon_polarity_task_loss</td><td>0.18609</td></tr><tr><td>batch_size</td><td>4</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>inner_lr</td><td>0.0</td></tr><tr><td>inner_steps</td><td>5</td></tr><tr><td>meta_loss</td><td>20.27886</td></tr><tr><td>meta_lr</td><td>1e-05</td></tr><tr><td>num_query</td><td>5</td></tr><tr><td>num_support</td><td>5</td></tr><tr><td>sentiment140_accuracy</td><td>0.79518</td></tr><tr><td>sentiment140_task_loss</td><td>4.64264</td></tr><tr><td>yelp_polarity_accuracy</td><td>0.8758</td></tr><tr><td>yelp_polarity_task_loss</td><td>0.22662</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">maml-multiset-manual</strong> at: <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/maml-fewshot-multiset/runs/s50nr2sa' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/maml-fewshot-multiset/runs/s50nr2sa</a><br> View project at: <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/maml-fewshot-multiset' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/maml-fewshot-multiset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250203_212735-s50nr2sa\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T13:11:15.895714Z",
     "start_time": "2025-02-06T13:11:15.235770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"./NLP_MAML_model\"\n",
    "adapted_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ],
   "id": "f48415434003600f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./NLP_MAML_model\\\\tokenizer_config.json',\n",
       " './NLP_MAML_model\\\\special_tokens_map.json',\n",
       " './NLP_MAML_model\\\\vocab.txt',\n",
       " './NLP_MAML_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e8cb3c476320572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6bf774641334adcf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
