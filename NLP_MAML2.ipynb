{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:31.579323Z",
     "start_time": "2025-01-24T21:58:18.996350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import wandb\n",
    "import optuna\n"
   ],
   "id": "daceca4ef779ee1f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:34.681026Z",
     "start_time": "2025-01-24T21:58:31.587316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"few-shot-yelp\",\n",
    "    name=\"maml-yelp-few-shot-optuna\",\n",
    ")"
   ],
   "id": "1416990c03018051",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kostic-stojan23 (kostic-stojan23-university-of-belgrade). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Korisnik\\Desktop\\RI_projekat\\wandb\\run-20250124_225832-98sia2uz</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz' target=\"_blank\">maml-yelp-few-shot-optuna</a></strong> to <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1b16c142810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:35.911336Z",
     "start_time": "2025-01-24T21:58:35.355442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"NLP_VER1\"\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "id": "274c0bd9e333514b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:43.248596Z",
     "start_time": "2025-01-24T21:58:35.961101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Yelp Polarity Dataset\n",
    "dataset = load_dataset(\"yelp_polarity\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ],
   "id": "318e13a06079d143",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:43.285568Z",
     "start_time": "2025-01-24T21:58:43.271566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Few-Shot Dataset\n",
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, data, num_support, num_query):\n",
    "        self.data = data\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "\n",
    "    def get_task(self):\n",
    "        indices = list(range(len(self.data)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        support_indices = indices[:self.num_support]\n",
    "        query_indices = indices[self.num_support:self.num_support + self.num_query]\n",
    "\n",
    "        support_set = [(self.data[i]['text'], self.data[i]['label']) for i in support_indices]\n",
    "        query_set = [(self.data[i]['text'], self.data[i]['label']) for i in query_indices]\n",
    "\n",
    "        return support_set, query_set"
   ],
   "id": "ffc8969d253b17fe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:43.322560Z",
     "start_time": "2025-01-24T21:58:43.310317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inner loop for task-specific adaptation\n",
    "def inner_loop(model, support_set, num_steps=1, lr=1e-5):\n",
    "    task_model = deepcopy(model)\n",
    "    optimizer = Adam(task_model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        for text, label in support_set:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            labels = torch.tensor([label])\n",
    "\n",
    "            outputs = task_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return task_model"
   ],
   "id": "a41157fd8ec918b0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:43.366845Z",
     "start_time": "2025-01-24T21:58:43.351659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Outer loop for meta-learning\n",
    "def outer_loop(meta_model, tasks, meta_optimizer, num_inner_steps=1):\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "\n",
    "    for support_set, query_set in tasks:\n",
    "        task_model = inner_loop(meta_model, support_set, num_steps=num_inner_steps)\n",
    "\n",
    "        for text, label in query_set:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            labels = torch.tensor([label])\n",
    "\n",
    "            outputs = task_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    return total_loss.item()"
   ],
   "id": "6a4da780199ebc16",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:58:43.407796Z",
     "start_time": "2025-01-24T21:58:43.391363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optuna objective function with W&B logging\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    meta_lr = trial.suggest_loguniform(\"meta_lr\", 1e-5, 1e-3)\n",
    "    inner_lr = trial.suggest_loguniform(\"inner_lr\", 1e-5, 1e-3)\n",
    "    num_support = trial.suggest_int(\"num_support\", 1, 10)\n",
    "    num_query = trial.suggest_int(\"num_query\", 1, 10)\n",
    "    inner_steps = trial.suggest_int(\"inner_steps\", 1, 5)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 2, 8)\n",
    "    meta_epochs = trial.suggest_int(\"meta_epochs\", 3, 5)\n",
    "\n",
    "    # Create Few-Shot Dataset\n",
    "    few_shot_dataset = FewShotDataset(train_data, num_support, num_query)\n",
    "\n",
    "    # Define meta-optimizer\n",
    "    meta_optimizer = Adam(base_model.parameters(), lr=meta_lr)\n",
    "\n",
    "    # Meta-training loop\n",
    "    for epoch in range(meta_epochs):\n",
    "        tasks = [few_shot_dataset.get_task() for _ in range(batch_size)]\n",
    "        loss = outer_loop(base_model, tasks, meta_optimizer, num_inner_steps=inner_steps)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"trial\": trial.number,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"meta_loss\": loss,\n",
    "            \"meta_lr\": meta_lr,\n",
    "            \"inner_lr\": inner_lr,\n",
    "            \"num_support\": num_support,\n",
    "            \"num_query\": num_query,\n",
    "            \"inner_steps\": inner_steps,\n",
    "            \"batch_size\": batch_size,\n",
    "        })\n",
    "\n",
    "    return loss"
   ],
   "id": "78027f21956fe0a3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T00:30:47.332538Z",
     "start_time": "2025-01-24T21:58:43.448016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optuna study setup and optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Log the best trial to W&B\n",
    "best_trial = study.best_trial\n",
    "wandb.log({\n",
    "    \"best_trial\": best_trial.number,\n",
    "    **best_trial.params,\n",
    "    \"best_loss\": best_trial.value,\n",
    "})\n",
    "\n",
    "# Meta-training with best parameters\n",
    "best_params = best_trial.params\n",
    "few_shot_dataset = FewShotDataset(train_data, best_params[\"num_support\"], best_params[\"num_query\"])\n",
    "meta_optimizer = Adam(base_model.parameters(), lr=best_params[\"meta_lr\"])\n",
    "\n",
    "for epoch in range(best_params[\"meta_epochs\"]):\n",
    "    tasks = [few_shot_dataset.get_task() for _ in range(best_params[\"batch_size\"])]\n",
    "    loss = outer_loop(base_model, tasks, meta_optimizer, num_inner_steps=best_params[\"inner_steps\"])\n",
    "    wandb.log({\"epoch\": epoch + 1, \"meta_loss\": loss})\n",
    "    print(f\"Epoch {epoch + 1}, Meta Loss: {loss:.4f}\")"
   ],
   "id": "b2747ba95e415a3b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-24 22:58:43,455] A new study created in memory with name: no-name-ac778edf-dd23-4515-922e-182905a61003\n",
      "C:\\Users\\Korisnik\\AppData\\Local\\Temp\\ipykernel_17844\\1405269402.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  meta_lr = trial.suggest_loguniform(\"meta_lr\", 1e-5, 1e-3)\n",
      "C:\\Users\\Korisnik\\AppData\\Local\\Temp\\ipykernel_17844\\1405269402.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  inner_lr = trial.suggest_loguniform(\"inner_lr\", 1e-5, 1e-3)\n",
      "[I 2025-01-24 23:09:00,472] Trial 0 finished with value: 23.132558822631836 and parameters: {'meta_lr': 0.0007043030472696078, 'inner_lr': 6.557933362066147e-05, 'num_support': 6, 'num_query': 10, 'inner_steps': 4, 'batch_size': 8, 'meta_epochs': 4}. Best is trial 0 with value: 23.132558822631836.\n",
      "[I 2025-01-24 23:12:42,890] Trial 1 finished with value: 1.9332340955734253 and parameters: {'meta_lr': 0.00012328944677160724, 'inner_lr': 0.00011600755636253694, 'num_support': 10, 'num_query': 9, 'inner_steps': 3, 'batch_size': 2, 'meta_epochs': 5}. Best is trial 1 with value: 1.9332340955734253.\n",
      "[I 2025-01-24 23:18:24,629] Trial 2 finished with value: 9.519462585449219 and parameters: {'meta_lr': 0.0005056738982375394, 'inner_lr': 4.63185013957147e-05, 'num_support': 4, 'num_query': 3, 'inner_steps': 4, 'batch_size': 7, 'meta_epochs': 4}. Best is trial 1 with value: 1.9332340955734253.\n",
      "[I 2025-01-24 23:19:03,995] Trial 3 finished with value: 1.4435416460037231 and parameters: {'meta_lr': 0.0005648956235336895, 'inner_lr': 1.5176227644068821e-05, 'num_support': 1, 'num_query': 3, 'inner_steps': 2, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:23:01,303] Trial 4 finished with value: 8.937413215637207 and parameters: {'meta_lr': 0.00021502792056777922, 'inner_lr': 0.0005573280838490494, 'num_support': 7, 'num_query': 5, 'inner_steps': 5, 'batch_size': 3, 'meta_epochs': 3}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:25:03,086] Trial 5 finished with value: 6.499984264373779 and parameters: {'meta_lr': 2.4643474463334687e-05, 'inner_lr': 0.00015078292540236132, 'num_support': 1, 'num_query': 6, 'inner_steps': 3, 'batch_size': 4, 'meta_epochs': 5}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:30:35,616] Trial 6 finished with value: 22.801488876342773 and parameters: {'meta_lr': 2.1096208932661014e-05, 'inner_lr': 0.00046347781093314405, 'num_support': 3, 'num_query': 9, 'inner_steps': 5, 'batch_size': 7, 'meta_epochs': 3}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:33:58,235] Trial 7 finished with value: 9.273545265197754 and parameters: {'meta_lr': 8.880460276787237e-05, 'inner_lr': 1.2335613549942658e-05, 'num_support': 10, 'num_query': 6, 'inner_steps': 1, 'batch_size': 5, 'meta_epochs': 4}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:38:19,512] Trial 8 finished with value: 12.431122779846191 and parameters: {'meta_lr': 2.476034140164772e-05, 'inner_lr': 0.0003655229541090772, 'num_support': 6, 'num_query': 6, 'inner_steps': 1, 'batch_size': 7, 'meta_epochs': 5}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:41:26,356] Trial 9 finished with value: 6.225011348724365 and parameters: {'meta_lr': 0.0008913733087430424, 'inner_lr': 5.402369154672439e-05, 'num_support': 4, 'num_query': 3, 'inner_steps': 3, 'batch_size': 6, 'meta_epochs': 3}. Best is trial 3 with value: 1.4435416460037231.\n",
      "[I 2025-01-24 23:41:45,039] Trial 10 finished with value: 0.021835437044501305 and parameters: {'meta_lr': 0.00026877451405112756, 'inner_lr': 1.0001160738244707e-05, 'num_support': 1, 'num_query': 1, 'inner_steps': 2, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:42:04,862] Trial 11 finished with value: 1.0919135808944702 and parameters: {'meta_lr': 0.00028059802653003496, 'inner_lr': 1.0464087645252247e-05, 'num_support': 1, 'num_query': 1, 'inner_steps': 2, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:42:37,306] Trial 12 finished with value: 0.23571795225143433 and parameters: {'meta_lr': 0.0002321883948099942, 'inner_lr': 2.3159662267099376e-05, 'num_support': 2, 'num_query': 1, 'inner_steps': 2, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:43:49,199] Trial 13 finished with value: 3.7975168228149414 and parameters: {'meta_lr': 6.713714839975184e-05, 'inner_lr': 2.5757641639255516e-05, 'num_support': 3, 'num_query': 1, 'inner_steps': 2, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:44:47,332] Trial 14 finished with value: 0.2016899138689041 and parameters: {'meta_lr': 0.00024084558084832867, 'inner_lr': 2.343676719322978e-05, 'num_support': 2, 'num_query': 1, 'inner_steps': 2, 'batch_size': 4, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:46:48,224] Trial 15 finished with value: 3.26962947845459 and parameters: {'meta_lr': 0.00013957240653424415, 'inner_lr': 2.6349044739860872e-05, 'num_support': 8, 'num_query': 4, 'inner_steps': 1, 'batch_size': 4, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:48:26,748] Trial 16 finished with value: 0.06613893061876297 and parameters: {'meta_lr': 5.004675840676222e-05, 'inner_lr': 0.00020553613722174973, 'num_support': 3, 'num_query': 2, 'inner_steps': 2, 'batch_size': 5, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:51:45,516] Trial 17 finished with value: 0.6549411416053772 and parameters: {'meta_lr': 4.992717155401258e-05, 'inner_lr': 0.00024174062402515368, 'num_support': 4, 'num_query': 2, 'inner_steps': 4, 'batch_size': 5, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:53:58,056] Trial 18 finished with value: 1.645569920539856 and parameters: {'meta_lr': 1.2479980517471578e-05, 'inner_lr': 0.0008066155479174712, 'num_support': 3, 'num_query': 4, 'inner_steps': 1, 'batch_size': 6, 'meta_epochs': 5}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-24 23:57:06,721] Trial 19 finished with value: 3.218980312347412 and parameters: {'meta_lr': 4.185641190330636e-05, 'inner_lr': 0.00020353940751721805, 'num_support': 5, 'num_query': 2, 'inner_steps': 2, 'batch_size': 6, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:03:21,897] Trial 20 finished with value: 13.642155647277832 and parameters: {'meta_lr': 4.185544457347054e-05, 'inner_lr': 9.450460020500541e-05, 'num_support': 2, 'num_query': 7, 'inner_steps': 3, 'batch_size': 8, 'meta_epochs': 5}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:04:27,006] Trial 21 finished with value: 3.7121169567108154 and parameters: {'meta_lr': 0.00040900353339118356, 'inner_lr': 1.6228811849663204e-05, 'num_support': 2, 'num_query': 2, 'inner_steps': 2, 'batch_size': 4, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:05:44,122] Trial 22 finished with value: 0.18646635115146637 and parameters: {'meta_lr': 0.00017456154904251834, 'inner_lr': 3.810252233886442e-05, 'num_support': 2, 'num_query': 1, 'inner_steps': 2, 'batch_size': 5, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:06:24,324] Trial 23 finished with value: 0.6332481503486633 and parameters: {'meta_lr': 0.00015620152438750113, 'inner_lr': 4.207734426496484e-05, 'num_support': 1, 'num_query': 2, 'inner_steps': 1, 'batch_size': 5, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:08:34,425] Trial 24 finished with value: 12.459693908691406 and parameters: {'meta_lr': 8.170351835752399e-05, 'inner_lr': 7.237705947960401e-05, 'num_support': 3, 'num_query': 4, 'inner_steps': 2, 'batch_size': 6, 'meta_epochs': 3}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:10:58,517] Trial 25 finished with value: 0.03601333498954773 and parameters: {'meta_lr': 0.00039724248918914515, 'inner_lr': 0.00027394430881110953, 'num_support': 5, 'num_query': 1, 'inner_steps': 3, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:14:12,825] Trial 26 finished with value: 0.2566814422607422 and parameters: {'meta_lr': 0.00027977725316452986, 'inner_lr': 0.00025624725921270054, 'num_support': 5, 'num_query': 3, 'inner_steps': 3, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:17:20,188] Trial 27 finished with value: 0.07818921655416489 and parameters: {'meta_lr': 0.00033726717381301176, 'inner_lr': 0.00015629678929416081, 'num_support': 8, 'num_query': 2, 'inner_steps': 3, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:20:26,146] Trial 28 finished with value: 0.027562575414776802 and parameters: {'meta_lr': 0.0009763138811988783, 'inner_lr': 0.00034615167308375936, 'num_support': 4, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:24:46,536] Trial 29 finished with value: 0.341631144285202 and parameters: {'meta_lr': 0.0009059335983467435, 'inner_lr': 0.0007323347907588882, 'num_support': 6, 'num_query': 8, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:28:18,592] Trial 30 finished with value: 0.3453190326690674 and parameters: {'meta_lr': 0.0006466456027517112, 'inner_lr': 0.0003384503859201987, 'num_support': 7, 'num_query': 5, 'inner_steps': 4, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 10 with value: 0.021835437044501305.\n",
      "[I 2025-01-25 00:31:33,510] Trial 31 finished with value: 0.016133828088641167 and parameters: {'meta_lr': 0.0004429214522218696, 'inner_lr': 0.000318996140881537, 'num_support': 5, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:34:52,878] Trial 32 finished with value: 3.3823986053466797 and parameters: {'meta_lr': 0.0004546732537128709, 'inner_lr': 0.00034146896222247324, 'num_support': 5, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:38:17,244] Trial 33 finished with value: 1.102265477180481 and parameters: {'meta_lr': 0.0007609281014305605, 'inner_lr': 0.0006385874571107825, 'num_support': 6, 'num_query': 1, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:41:15,691] Trial 34 finished with value: 0.0838128924369812 and parameters: {'meta_lr': 0.00039015878692535787, 'inner_lr': 0.0009598243743909674, 'num_support': 4, 'num_query': 3, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:50:11,911] Trial 35 finished with value: 3.437788724899292 and parameters: {'meta_lr': 0.0005697266529932391, 'inner_lr': 0.0004305353679049593, 'num_support': 7, 'num_query': 10, 'inner_steps': 5, 'batch_size': 4, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:51:54,033] Trial 36 finished with value: 1.3845828771591187 and parameters: {'meta_lr': 0.0006658858072844072, 'inner_lr': 0.00011592167831250987, 'num_support': 5, 'num_query': 2, 'inner_steps': 3, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 31 with value: 0.016133828088641167.\n",
      "[I 2025-01-25 00:55:03,917] Trial 37 finished with value: 0.005342430900782347 and parameters: {'meta_lr': 0.0005025968791505043, 'inner_lr': 0.000546321717373677, 'num_support': 4, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 5}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:04:14,934] Trial 38 finished with value: 3.0016934871673584 and parameters: {'meta_lr': 0.0009597688552367335, 'inner_lr': 0.0005255660616607977, 'num_support': 9, 'num_query': 3, 'inner_steps': 4, 'batch_size': 4, 'meta_epochs': 5}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:06:50,479] Trial 39 finished with value: 3.0100574493408203 and parameters: {'meta_lr': 0.0005090334929339798, 'inner_lr': 0.00017673279003424977, 'num_support': 4, 'num_query': 3, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 5}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:12:08,544] Trial 40 finished with value: 0.07956444472074509 and parameters: {'meta_lr': 0.00032452551770314044, 'inner_lr': 8.673658954175014e-05, 'num_support': 6, 'num_query': 2, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 5}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:15:14,554] Trial 41 finished with value: 1.4421000480651855 and parameters: {'meta_lr': 0.0005040457106897048, 'inner_lr': 0.0002997913763790686, 'num_support': 5, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:17:49,359] Trial 42 finished with value: 0.026257313787937164 and parameters: {'meta_lr': 0.0006630044885659203, 'inner_lr': 0.00042751494410177195, 'num_support': 4, 'num_query': 1, 'inner_steps': 4, 'batch_size': 3, 'meta_epochs': 4}. Best is trial 37 with value: 0.005342430900782347.\n",
      "[I 2025-01-25 01:19:35,882] Trial 43 finished with value: 0.003125594463199377 and parameters: {'meta_lr': 0.0007234059302330797, 'inner_lr': 0.00044486134978413493, 'num_support': 4, 'num_query': 1, 'inner_steps': 4, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:21:24,030] Trial 44 finished with value: 0.018750324845314026 and parameters: {'meta_lr': 0.0007582253352342744, 'inner_lr': 0.0004989448592830366, 'num_support': 3, 'num_query': 2, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:23:19,532] Trial 45 finished with value: 0.030134061351418495 and parameters: {'meta_lr': 0.0007561435314381719, 'inner_lr': 0.0006224613887409883, 'num_support': 3, 'num_query': 2, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 5}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:23:54,251] Trial 46 finished with value: 0.01764732040464878 and parameters: {'meta_lr': 0.00019218944431335342, 'inner_lr': 0.0005102456086452478, 'num_support': 1, 'num_query': 1, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:26:20,320] Trial 47 finished with value: 0.05847356840968132 and parameters: {'meta_lr': 0.00017728566133703844, 'inner_lr': 0.0005115070938749284, 'num_support': 4, 'num_query': 2, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:28:27,307] Trial 48 finished with value: 3.0558767318725586 and parameters: {'meta_lr': 0.0001207092310243741, 'inner_lr': 0.0007283935321149734, 'num_support': 3, 'num_query': 9, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n",
      "[I 2025-01-25 01:29:07,130] Trial 49 finished with value: 0.04224070534110069 and parameters: {'meta_lr': 0.0005527364923045716, 'inner_lr': 0.0008873051871429393, 'num_support': 1, 'num_query': 1, 'inner_steps': 5, 'batch_size': 2, 'meta_epochs': 4}. Best is trial 43 with value: 0.003125594463199377.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Meta Loss: 0.0162\n",
      "Epoch 2, Meta Loss: 0.0075\n",
      "Epoch 3, Meta Loss: 1.4160\n",
      "Epoch 4, Meta Loss: 0.0067\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T00:30:56.032773Z",
     "start_time": "2025-01-25T00:30:47.476505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on a new task\n",
    "new_task_data = FewShotDataset(test_data, best_params[\"num_support\"], best_params[\"num_query\"])\n",
    "new_support_set, new_query_set = new_task_data.get_task()\n",
    "\n",
    "adapted_model = inner_loop(base_model, new_support_set, num_steps=best_params[\"inner_steps\"])\n",
    "total_loss = 0\n",
    "\n",
    "for text, label in new_query_set:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    labels = torch.tensor([label])\n",
    "\n",
    "    outputs = adapted_model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss\n",
    "\n",
    "wandb.log({\"new_task_loss\": total_loss.item()})\n",
    "print(f\"New Task Evaluation Loss: {total_loss.item():.4f}\")"
   ],
   "id": "fc6d84f66e0f1ed9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Task Evaluation Loss: 0.0010\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T00:30:58.546560Z",
     "start_time": "2025-01-25T00:30:56.061742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Classification metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for text, label in new_query_set:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = adapted_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    true_labels.append(label)\n",
    "    predicted_labels.append(predictions.item())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(true_labels, predicted_labels))\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "9a01da96841de3db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁▂▃▇▅▇▇▁▁▁▁▂▃▃▅██▅▅▅▂▂▂▂▂▂▃▃▃▁▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>best_loss</td><td>▁</td></tr><tr><td>best_trial</td><td>▁</td></tr><tr><td>epoch</td><td>▁▅▁█▃▅▅▃▅▁▁▁█▅▁▅▃▆▅▁▆▃▆▅▆▁▆▆█▃▆▁▃▅▁▁▆▅▁▁</td></tr><tr><td>inner_lr</td><td>▂▁▁▁▆▅▅▁▁▁▁▃▃▃█▂▁▁▂▂▃▃▃▂▄▄▄▄▇▅▆▆▂▂▂▄▄▅▇▅</td></tr><tr><td>inner_steps</td><td>▆▆▃▃▃▁▁▁▅▅▃▁▁▃▆▃▃▃▅▅▅▆▆▆▆▆▆▆██▆▆▆▆▆█████</td></tr><tr><td>meta_epochs</td><td>▁</td></tr><tr><td>meta_loss</td><td>█▆▁▁▂▁▄▂▁▁▂▁▁▁▁▅▂▁▁▂▁▁▁▁▅▁▁▁▂▂▂▂▁▁▁▁▂▁▃▁</td></tr><tr><td>meta_lr</td><td>▆▂▂▅▅▁▃▃▃▃▁▃▃▂▁▁▄▂▂▄██▆▆▄▄▄▄▅▅▅▅▃▃▅▆▇▇▇▅</td></tr><tr><td>new_task_loss</td><td>▁</td></tr><tr><td>num_query</td><td>█▇▇▃▅▅▅▅▁▁▁▁▁▃▂▆▁▁▂▃▂▁▄▄▁▁▁▁▃█▁▃▂▁▁▂▂▂▁▁</td></tr><tr><td>num_support</td><td>▅▅▅█▁██▃▂▂▃▃▃▃▄▂▂▂▂▄▃▅▆▆▄▅▅▃▆▃▇▃▃▅▄▃▃▃▃▁</td></tr><tr><td>trial</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>2</td></tr><tr><td>best_loss</td><td>0.00313</td></tr><tr><td>best_trial</td><td>43</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>inner_lr</td><td>0.00044</td></tr><tr><td>inner_steps</td><td>4</td></tr><tr><td>meta_epochs</td><td>4</td></tr><tr><td>meta_loss</td><td>0.00671</td></tr><tr><td>meta_lr</td><td>0.00072</td></tr><tr><td>new_task_loss</td><td>0.001</td></tr><tr><td>num_query</td><td>1</td></tr><tr><td>num_support</td><td>4</td></tr><tr><td>trial</td><td>49</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">maml-yelp-few-shot-optuna</strong> at: <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp/runs/98sia2uz</a><br> View project at: <a href='https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp' target=\"_blank\">https://wandb.ai/kostic-stojan23-university-of-belgrade/few-shot-yelp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250124_225832-98sia2uz\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T08:35:55.956585Z",
     "start_time": "2025-01-25T08:35:55.668802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define file paths for saving\n",
    "model_save_path = \"maml_adapted_model.pth\"\n",
    "tokenizer_save_path = \"maml_tokenizer/\"\n",
    "\n",
    "# Save the adapted model\n",
    "torch.save(adapted_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved at {tokenizer_save_path}\")"
   ],
   "id": "9fbe878a10a9d87f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at maml_adapted_model.pth\n",
      "Tokenizer saved at maml_tokenizer/\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f3a8f2311ca75f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
